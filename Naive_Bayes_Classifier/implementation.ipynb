{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u8bCw0D1FwN6"
   },
   "source": [
    "# Naive Bayes Classifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3xEMW7V_FwN7"
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HDVb7r2OFwN7"
   },
   "outputs": [],
   "source": [
    "# This function prepares the data by reading it from a file and converting it \n",
    "# into a useful format for training and testing\n",
    "\n",
    "def preprocess():\n",
    "    \n",
    "    # Read the files\n",
    "    trainset = pd.read_csv(\"train.csv\", header=None)\n",
    "    testset = pd.read_csv(\"test.csv\", header=None)\n",
    "    \n",
    "    # Identify 9999 value as Null\n",
    "    trainset = trainset.replace(9999, np.NaN)\n",
    "    testset = testset.replace(9999, np.NaN)\n",
    "    \n",
    "    # Calculate medians for all pose categories\n",
    "    medians = trainset.groupby(trainset[0]).median()\n",
    "\n",
    "    # Median imputation for null value\n",
    "    for i in range(trainset.shape[0]):\n",
    "      for j in range(1, trainset.shape[1]):\n",
    "        if (np.isnan(trainset.loc[i, j])):\n",
    "            trainset.loc[i, j] = medians.loc[trainset.loc[i, 0], j]\n",
    "\n",
    "    return [trainset, testset]\n",
    "\n",
    "trainset = preprocess()[0]\n",
    "testset = preprocess()[1]\n",
    "print(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Q9Cy0byugsPF"
   },
   "outputs": [],
   "source": [
    "# This function calculates prior probabilities and likelihoods from the training data and using\n",
    "# them to build a naive Bayes model\n",
    "\n",
    "def train(trainset):\n",
    "    \n",
    "    # Extract all pose categories\n",
    "    categories = list(trainset[0].drop_duplicates())\n",
    "    instance_num = trainset.shape[0]\n",
    "\n",
    "    prior = defaultdict(float)\n",
    "    mean = pd.DataFrame()\n",
    "    var = pd.DataFrame()\n",
    "\n",
    "    # For each data point in each category, calculate its mean and variance\n",
    "    for category in categories:\n",
    "        curr_category = trainset[trainset[0] == category]\n",
    "        prior[category] = curr_category.shape[0] / instance_num\n",
    "        mean[category] = curr_category.iloc[:, 1: 23].mean()\n",
    "        var[category] = curr_category.iloc[:, 1: 23].var()\n",
    "    \n",
    "    train_model = [prior, mean, var]\n",
    "    return train_model\n",
    "\n",
    "train_model = train(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "myapp-Pk0rW4"
   },
   "outputs": [],
   "source": [
    "# This function calculates the corresponding y value in a normal distribution\n",
    "# given mean, variance and x value\n",
    "\n",
    "def calc_normal(mean, var, x):\n",
    "    coefficient = 1 / np.sqrt(2 * math.pi * var) \n",
    "    y =  coefficient * np.exp(- np.power((x - mean), 2) / (2 * var))\n",
    "\n",
    "    return np.log(y, where = y > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "-ary_wdtyaVn"
   },
   "outputs": [],
   "source": [
    "# This function predicts classes for new items in a test dataset (for the purposes of this assignment, you\n",
    "# can re-use the training data as a test set)\n",
    "\n",
    "def predict(testset, train_model):\n",
    "    [prior, mean, var] = train_model\n",
    "    predict_result = []\n",
    "\n",
    "    for i in range(testset.shape[0]):\n",
    "        test = testset.iloc[i, 1:].astype(float)\n",
    "        result = pd.Series(dtype='float64')\n",
    "        max_score = 0\n",
    "        \n",
    "        # Calculate the score for each category\n",
    "        for category in train_model[0].keys():\n",
    "            result[category] = 0\n",
    "            \n",
    "            # Only include values that are not null\n",
    "            for i in range(1, len(test) + 1):\n",
    "                if not np.isnan(test[i]):\n",
    "                    result[category] += calc_normal(mean[category][i], var[category][i], test[i])\n",
    "            \n",
    "            # Add prior probability\n",
    "            result[category] += math.log(prior[category])\n",
    "            \n",
    "            # Check if the current class scores the highest\n",
    "            if max_score == 0 or result[category] > max_score:\n",
    "                max_score = result[category]\n",
    "                result_category = category\n",
    "        \n",
    "        # Append the result to the result list\n",
    "        predict_result.append(result_category)\n",
    "        \n",
    "    return predict_result\n",
    "\n",
    "predict_result = predict(testset, train_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m0pZuwmmFwN8",
    "outputId": "2965d2cc-9661-4f15-94ad-da77dd395507"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy: 0.7413793103448276\n"
     ]
    }
   ],
   "source": [
    "# This function should evaliate the prediction performance by comparing your model’s class outputs to ground\n",
    "# truth labels\n",
    "\n",
    "def evaluate(predict_result, testset):\n",
    "    correct_num = 0\n",
    "    correct_result = testset.iloc[:, 0].tolist()\n",
    "    \n",
    "    # Calculate how much times the classifier predicts correctly\n",
    "    for i in range(len(predict_result)):\n",
    "        if (correct_result[i] == predict_result[i]):\n",
    "            correct_num += 1\n",
    "\n",
    "    # Return the accuracy\n",
    "    return correct_num / len(predict_result)\n",
    "\n",
    "accuracy = evaluate(predict_result, testset)\n",
    "print(\"Overall accuracy: \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "87eGbeY-FwN8"
   },
   "source": [
    "## Questions \n",
    "\n",
    "\n",
    "If you are in a group of 1, you will respond to **two** questions of your choosing.\n",
    "\n",
    "If you are in a group of 2, you will respond to **four** questions of your choosing.\n",
    "\n",
    "A response to a question should take about 100–250 words, and make reference to the data wherever possible.\n",
    "\n",
    "#### NOTE: you may develope codes or functions to help respond to the question here, but your formal answer should be submitted separately as a PDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DufbvL2pFwN9"
   },
   "source": [
    "### Q1\n",
    "Since this is a multiclass classification problem, there are multiple ways to compute precision, recall, and F-score for this classifier. Implement at least two of the methods from the \"Model Evaluation\" lecture and discuss any differences between them. (The implementation should be your own and should not just call a pre-existing function.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "id": "wD_L5cGvFwN9",
    "outputId": "8e5873dd-2c55-4d3b-98b5-077c053f12c9"
   },
   "outputs": [],
   "source": [
    "# This function will return a dictionary of results containing the values of \n",
    "# accuracy, precision, recall and F-score for each class respectively\n",
    "\n",
    "def calc_result(predict, correct, all_classes):\n",
    "    class_weight = {}\n",
    "    basic_result = {}\n",
    "    computed_result = {}\n",
    "    \n",
    "    # Calculate statistics for each class respectively\n",
    "    for curr_class in all_classes:\n",
    "        tp = tn = fp = fn = number = 0\n",
    "        \n",
    "        # Check each predict result and classify into tp, fn, fp, tn\n",
    "        for i in range(len(predict)):\n",
    "            if (correct[i] == curr_class):\n",
    "                number += 1\n",
    "            if (correct[i] == curr_class and predict[i] == curr_class):\n",
    "                tp += 1\n",
    "            elif (correct[i] == curr_class and predict[i] != curr_class):\n",
    "                fn += 1\n",
    "            elif (correct[i] != curr_class and predict[i] == curr_class):\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        \n",
    "        class_weight[curr_class] = number\n",
    "        basic_result[curr_class] = [tp, tn, fp, fn]\n",
    "        \n",
    "        # Calculate accuracy, precision, recall, F-score\n",
    "        accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "        precision = tp / (tp + fp)\n",
    "        recall = tp / (tp + fn)\n",
    "        f_score = 2 * precision * recall / (precision + recall)\n",
    "        computed_result[curr_class] = [accuracy, precision, recall, f_score]\n",
    "\n",
    "    return [class_weight, basic_result, computed_result]\n",
    "\n",
    "result = calc_result(predict_result, testset.iloc[:, 0].tolist(), train_model[0].keys())\n",
    "class_weight = result[0]\n",
    "basic_result = result[1]\n",
    "computed_result = result[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "L6NlHsPtPYgo"
   },
   "outputs": [],
   "source": [
    "# Plot basic results\n",
    "\n",
    "column_names = [\"tp\", \"tn\", \"fp\", \"fn\"]\n",
    "row_names = list(train_model[0].keys())\n",
    "results = list(basic_result.values())\n",
    "\n",
    "tab = plt.table(cellText = results, colLabels = column_names, rowLabels = row_names,\n",
    "              loc = 'center', cellLoc = 'center', rowLoc = 'center')\n",
    "plt.axis('off')\n",
    "plt.savefig(\"basic_results.png\", dpi = 500, bbox_inches = 'tight')\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "pxNhI5gmLYVe"
   },
   "outputs": [],
   "source": [
    "# Plot calculated results\n",
    "\n",
    "column_names = [\"Accuracy\", \"Precision\", \"Recall\", \"F-score\"]\n",
    "row_names = list(train_model[0].keys())\n",
    "results = list(computed_result.values())\n",
    "\n",
    "tab = plt.table(cellText = results, colLabels = column_names, rowLabels = row_names,\n",
    "              loc = 'center', cellLoc = 'center', rowLoc = 'center')\n",
    "plt.axis('off')\n",
    "plt.savefig(\"computed_results.png\", dpi = 500, bbox_inches = 'tight')\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "u0fhIK0kUZzW"
   },
   "outputs": [],
   "source": [
    "# Macro-averaging\n",
    "\n",
    "macro_precision = macro_recall = 0\n",
    "for curr_class in computed_result.keys():\n",
    "    macro_precision += computed_result[curr_class][1]\n",
    "    macro_recall += computed_result[curr_class][2]\n",
    "\n",
    "macro_precision /= len(computed_result.keys())\n",
    "macro_recall /= len(computed_result.keys())\n",
    "macro_f_score = 2 * macro_precision * macro_recall / (macro_precision + macro_recall)\n",
    "macro_result = [macro_precision, macro_recall, macro_f_score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "AZynhV6uC8Ck"
   },
   "outputs": [],
   "source": [
    "# Micro-averaging\n",
    "\n",
    "total_tp = total_tn = total_fp = total_fn = 0\n",
    "for curr_class in basic_result.keys():\n",
    "    total_tp += basic_result[curr_class][0]\n",
    "    total_tn += basic_result[curr_class][1]\n",
    "    total_fp += basic_result[curr_class][2]\n",
    "    total_fn += basic_result[curr_class][3]\n",
    "\n",
    "micro_precision = total_tp / (total_tp + total_fp)\n",
    "micro_recall = total_tp / (total_tp + total_fn)\n",
    "micro_f_score = 2 * micro_precision * micro_recall / (micro_precision + micro_recall)\n",
    "micro_result = [micro_precision, micro_recall, micro_f_score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Hxqo2uPoDBiP"
   },
   "outputs": [],
   "source": [
    "# Weighted averaging\n",
    "weighted_precision = weighted_recall = 0\n",
    "for curr_class in computed_result.keys():\n",
    "    weighted_precision += computed_result[curr_class][1] * (class_weight[curr_class] / testset.shape[0])\n",
    "    weighted_recall += computed_result[curr_class][2] * (class_weight[curr_class] / testset.shape[0])\n",
    "weighted_f_score = 2 * weighted_precision * weighted_recall / (weighted_precision + weighted_recall)\n",
    "\n",
    "weighted_result = [weighted_precision, weighted_recall, weighted_f_score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "hAjwKqLgDF_r"
   },
   "outputs": [],
   "source": [
    "# Plot overall precision and recall\n",
    "\n",
    "x = np.arange(3)\n",
    "precision = [macro_result[0], micro_result[0], weighted_result[0]]\n",
    "recall = [macro_result[1], micro_result[1], weighted_result[1]]\n",
    "\n",
    "bar_width = 0.35\n",
    "tick_label = [\"Macro\", \"Micro\", \"Weighted\"]\n",
    "plt.bar(x, precision, bar_width, color=\"c\", align=\"center\", label=\"Precision\", alpha=0.5)\n",
    "plt.bar(x + bar_width, recall, bar_width, color=\"b\", align=\"center\", label=\"Recall\", alpha=0.5)\n",
    "plt.ylim(0, 1.2)\n",
    "\n",
    "plt.title(\"Precision and Recall based on Three Calculation Methods\")\n",
    "plt.xlabel(\"Different Methods to compute statistics\")\n",
    "plt.ylabel(\"Results\")\n",
    "\n",
    "plt.xticks(x + bar_width / 2, tick_label)\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(\"pre_recall.png\", dpi = 500, bbox_inches = 'tight')\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "vUwfvu81iL5l"
   },
   "outputs": [],
   "source": [
    "# Plot overall F-score\n",
    "\n",
    "f_score = [macro_result[2], micro_result[2], weighted_result[2]]\n",
    "plt.bar(x, f_score, bar_width, color='r', align=\"center\")\n",
    "plt.xticks(x, tick_label)\n",
    "plt.ylim(0, 1.2)\n",
    "\n",
    "plt.title(\"F-score based on Three Calculation Methods\")\n",
    "plt.xlabel(\"Different Methods to compute statistics\")\n",
    "plt.ylabel(\"Results\")\n",
    "\n",
    "plt.savefig(\"f_score.png\", dpi = 500, bbox_inches = 'tight')\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8d9DCh5aFwN9"
   },
   "source": [
    "### Q2\n",
    "The Gaussian naıve Bayes classifier assumes that numeric attributes come from a Gaussian distribution. Is this assumption always true for the numeric attributes in this dataset? Identify some cases where the Gaussian assumption is violated and describe any evidence (or lack thereof) that this has some effect on the classifier’s predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "dP55nFdAFwN-",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D7aHQpMfFwN-"
   },
   "source": [
    "### Q3\n",
    "Implement a kernel density estimate (KDE) naive Bayes classifier and compare its performance to the Gaussian naive Bayes classifier. Recall that KDE has kernel bandwidth as a free parameter -- you can choose an arbitrary value for this, but a value in the range 5-25 is recommended. Discuss any differences you observe between the Gaussian and KDE naive Bayes classifiers. (As with the Gaussian naive Bayes, this KDE naive Bayes implementation should be your own and should not just call a pre-existing function.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "EPRD9r6Wc7-D"
   },
   "outputs": [],
   "source": [
    "# This function calculates the normal pdf value\n",
    "\n",
    "def normal_kernel(x, mean, sigma):\n",
    "    \n",
    "    # Handle missing value\n",
    "    if (x == -2000 or mean == -2000):\n",
    "        return 1e-40\n",
    "    \n",
    "    return (math.e ** (-0.5 * (((x - mean) / sigma) ** 2))) / (math.sqrt(2 * math.pi) * sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "_z0veRrO0Wh6"
   },
   "outputs": [],
   "source": [
    "# This function uses the trainset to develop the kde prediction model\n",
    "\n",
    "def train_kde(categories, trainset):\n",
    "    model = dict()\n",
    "    for category in categories:\n",
    "        \n",
    "        # Calculate prior probability\n",
    "        prior = trainset.groupby(0).size()[category] / len(trainset)\n",
    "        \n",
    "        # Extract all items\n",
    "        model[category] = np.array([prior, trainset[trainset[0] == category].iloc[:,1:].to_numpy()], dtype=object)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "l8BE7-B8e2xk"
   },
   "outputs": [],
   "source": [
    "# This function predicts testset pose based on kde naive bayes\n",
    "\n",
    "def predict_kde(trainset, testset, h):\n",
    "    \n",
    "    # Extract all categories\n",
    "    categories = np.unique(trainset[0])\n",
    "\n",
    "    # Train kde model\n",
    "    kde_model = train_kde(categories, trainset)\n",
    "    \n",
    "    result = []\n",
    "\n",
    "    # Predict for each instance in testset\n",
    "    for instance in testset:    \n",
    "        probs = dict()\n",
    "\n",
    "        # Calculate probability for each pose\n",
    "        for category in categories:\n",
    "            \n",
    "            # Extracrt all instances from trainset\n",
    "            samples = kde_model[category][1]\n",
    "\n",
    "            # Add prior probability\n",
    "            prob_sum = np.log(kde_model[category][0],\n",
    "                              where = kde_model[category][0] > 0)\n",
    "            \n",
    "            # Calculate probability for each attributes\n",
    "            for i in range(len(instance)):\n",
    "                length = 0\n",
    "                score = 0\n",
    "                test_atr = instance[i]\n",
    "                train_atr = samples[:, i]\n",
    "                \n",
    "                # Calculate probability for each attributes for each instance\n",
    "                for one_atr in train_atr:\n",
    "                    # Skip missing values\n",
    "                    if (one_atr != -2000):\n",
    "                        length += 1\n",
    "                    score += normal_kernel(test_atr, one_atr, h)\n",
    "                \n",
    "                # Smoothing\n",
    "                if score == 0: score = 1e-40\n",
    "                if length == 0: length = 1\n",
    "                score /= length \n",
    "                prob_sum += np.log(score, where = score > 0)\n",
    "                \n",
    "            probs[category] = prob_sum\n",
    "\n",
    "        predict = max(probs, key = probs.get)\n",
    "        result.append(predict)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nda6Y6L8f8iA",
    "outputId": "66e64beb-5787-4f4a-b84e-a73c23c68043"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy: 0.7758620689655172\n"
     ]
    }
   ],
   "source": [
    "# Main section for Q3\n",
    "trainset = pd.read_csv(\"train.csv\", header=None)\n",
    "testset = pd.read_csv(\"test.csv\", header=None)\n",
    "\n",
    "# Use -2000 to represent missing value\n",
    "trainset = trainset.replace(9999, -2000)\n",
    "testset = testset.replace(9999, -2000)\n",
    "\n",
    "# Arbitrary bandwidth is chosen\n",
    "h = 10\n",
    "\n",
    "# Predict and evaluate\n",
    "predict_result = predict_kde(trainset, testset.iloc[:,1:23].to_numpy(), h)\n",
    "accuracy = evaluate(predict_result, testset)\n",
    "print(\"Overall accuracy: \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQcneYJiFwN-"
   },
   "source": [
    "### Q4\n",
    "Instead of using an arbitrary kernel bandwidth for the KDE naive Bayes classifier, use random hold-out or cross-validation to choose the kernel bandwidth. Discuss how this changes the model performance compared to using an arbitrary kernel bandwidth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L3VY9xoDFwN-",
    "outputId": "e50c7331-cc0b-4f93-dedc-6dacf536f85e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For kernel bandwidth h = 5, the average accuracy is 0.7751\n",
      "For kernel bandwidth h = 6, the average accuracy is 0.7724\n",
      "For kernel bandwidth h = 7, the average accuracy is 0.7791\n",
      "For kernel bandwidth h = 8, the average accuracy is 0.7778\n",
      "For kernel bandwidth h = 9, the average accuracy is 0.7858\n",
      "For kernel bandwidth h = 10, the average accuracy is 0.7791\n",
      "For kernel bandwidth h = 11, the average accuracy is 0.7764\n",
      "For kernel bandwidth h = 12, the average accuracy is 0.7778\n",
      "For kernel bandwidth h = 13, the average accuracy is 0.7791\n",
      "For kernel bandwidth h = 14, the average accuracy is 0.7737\n",
      "For kernel bandwidth h = 15, the average accuracy is 0.7724\n",
      "For kernel bandwidth h = 16, the average accuracy is 0.7724\n",
      "For kernel bandwidth h = 17, the average accuracy is 0.7684\n",
      "For kernel bandwidth h = 18, the average accuracy is 0.7657\n",
      "For kernel bandwidth h = 19, the average accuracy is 0.7671\n",
      "For kernel bandwidth h = 20, the average accuracy is 0.7630\n",
      "For kernel bandwidth h = 21, the average accuracy is 0.7604\n",
      "For kernel bandwidth h = 22, the average accuracy is 0.7550\n",
      "For kernel bandwidth h = 23, the average accuracy is 0.7523\n",
      "For kernel bandwidth h = 24, the average accuracy is 0.7510\n",
      "For kernel bandwidth h = 25, the average accuracy is 0.7483\n",
      "The optimal kernel bandwidth for current trainset is 9\n"
     ]
    }
   ],
   "source": [
    "# It will take several minutes to run this block\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "trainset = pd.read_csv(\"train.csv\", header=None)\n",
    "testset = pd.read_csv(\"test.csv\", header=None)\n",
    "\n",
    "# Shuffle the trainset\n",
    "trainset = trainset.sample(frac=1, random_state = 80).reset_index(drop = True)\n",
    "\n",
    "# Accuracies list for all bandwidth\n",
    "accuracies = []\n",
    "\n",
    "# Each round use 80% of trainset as train and 20% as test\n",
    "kf = KFold(n_splits = 5)\n",
    "\n",
    "for h in range(5, 26):\n",
    "    total_accuracy = 0\n",
    "    \n",
    "    # Cross-validation for 5 folds\n",
    "    for train_indices, test_indices in kf.split(trainset):\n",
    "        \n",
    "        # Prepare the trainset and testset\n",
    "        one_train = trainset.iloc[train_indices,: ]\n",
    "        one_test = trainset.iloc[test_indices,: ]\n",
    "        one_train = one_train.reset_index(drop=True)\n",
    "        one_test = one_test.reset_index(drop=True)\n",
    "        \n",
    "        # Predict testset\n",
    "        predict = predict_kde(one_train, one_test.iloc[:,1:23].to_numpy(), h)\n",
    "        result = evaluate(predict, one_test)\n",
    "        total_accuracy += result \n",
    "    \n",
    "\n",
    "    ave_accuracy = total_accuracy / 5\n",
    "    print(\"For kernel bandwidth h = %d, the average accuracy is %.4f\"\n",
    "     % (h, ave_accuracy))\n",
    "\n",
    "    accuracies.append(ave_accuracy)\n",
    "    \n",
    "print('The optimal kernel bandwidth for current trainset is %d' % (np.argmax(accuracies) + 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A-ExDgC3FwN-"
   },
   "source": [
    "### Q5\n",
    "Naive Bayes ignores missing values, but in pose recognition tasks the missing values can be informative. Missing values indicate that some part of the body was obscured and sometimes this is relevant to the pose (e.g., holding one hand behind the back). Are missing values useful for this task? Implement a method that incorporates information about missing values and demonstrate whether it changes the classification results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "4bFTxPD6FwN_"
   },
   "outputs": [],
   "source": [
    "# Calculate the number of missing values in different classes\n",
    "\n",
    "trainset = pd.read_csv(\"train.csv\", header=None)\n",
    "testset = pd.read_csv(\"test.csv\", header=None)\n",
    "trainset = trainset.replace(9999, np.NaN)\n",
    "testset = testset.replace(9999, np.NaN)\n",
    "class_num = defaultdict(int)\n",
    "nan_num = defaultdict(int)\n",
    "\n",
    "for i in range(trainset.shape[0]):\n",
    "    class_num[trainset.iloc[i, 0]] += 1\n",
    "    curr_instance = trainset.iloc[i, 1:].astype(float)\n",
    "    for data in curr_instance:\n",
    "        if np.isnan(data):\n",
    "            nan_num[trainset.iloc[i, 0]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "5M9RBbWh5tWC"
   },
   "outputs": [],
   "source": [
    "# Separate all classes into two categories, i.e. less_missing and more_missing\n",
    "\n",
    "less_missing = []\n",
    "more_missing = []\n",
    "\n",
    "for curr_class in class_num.keys():\n",
    "    class_num[curr_class] = nan_num[curr_class] / class_num[curr_class]\n",
    "    if class_num[curr_class] > 2:\n",
    "        more_missing.append(curr_class)\n",
    "    else:\n",
    "        less_missing.append(curr_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the predict function\n",
    "\n",
    "def predict_miss(testset, train_model):\n",
    "    \n",
    "    # Identify 9999 value as Null\n",
    "    testset = testset.replace(9999, np.NaN)\n",
    "    [prior, mean, var] = train_model\n",
    "    predict_result = []\n",
    "\n",
    "    for i in range(testset.shape[0]):\n",
    "        test = testset.iloc[i, 1:].astype(float)\n",
    "        result = pd.Series(dtype='float64')\n",
    "\n",
    "        # Find category\n",
    "        nan_num = 0\n",
    "        for i in range(1, len(test) + 1):\n",
    "            if np.isnan(test[i]):\n",
    "                nan_num += 1\n",
    "        if nan_num > 2:\n",
    "            class_group = more_missing\n",
    "        else:\n",
    "            class_group = less_missing\n",
    "\n",
    "        # Find the class that has the highest score\n",
    "        max_score = 0\n",
    "        for category in class_group:\n",
    "            result[category] = 0\n",
    "            \n",
    "            # Only include values that are not null\n",
    "            for i in range(1, len(test) + 1):\n",
    "                if not np.isnan(test[i]):\n",
    "                    result[category] += calc_normal(mean[category][i], var[category][i], test[i])\n",
    "            result[category] += math.log(prior[category])\n",
    "            \n",
    "            # Check if the current class scores the highest\n",
    "            if max_score == 0 or result[category] > max_score:\n",
    "                max_score = result[category]\n",
    "                result_category = category\n",
    "\n",
    "        predict_result.append(result_category)\n",
    "        \n",
    "    return predict_result\n",
    "\n",
    "predict_result = predict_miss(testset, train_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy: 0.7068965517241379\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluate(predict_result, testset)\n",
    "print(\"Overall accuracy: \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yubd1ZwGFwN_"
   },
   "source": [
    "### Q6\n",
    "Engineer your own pose features from the provided keypoints. Instead of using the (x,y) positions of keypoints, you might consider the angles of the limbs or body, or the distances between pairs of keypoints. How does a naive Bayes classifier based on your engineered features compare to the classifier using (x,y) values? Please note that we are interested in explainable features for pose recognition, so simply putting the (x,y) values in a neural network or similar to get an arbitrary embedding will not receive full credit for this question. You should be able to explain the rationale behind your proposed features. Also, don't forget the conditional independence assumption of naive Bayes when proposing new features -- a large set of highly-correlated features may not work well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "wBHtz6DlFwN_",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "COMP30027_2021_assignment1_template.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
