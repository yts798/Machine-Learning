{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ipynb file contains my implementation of project 2\n",
    "\n",
    "First I am going to load package and data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import package\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import scipy\n",
    "import pickle\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "trainset = pd.read_csv(r'COMP30027_2021_Project2_datasets/recipe_train.csv')\n",
    "testset = pd.read_csv(r'COMP30027_2021_Project2_datasets/recipe_test.csv')\n",
    "trainse_t_np = trainset.to_numpy()\n",
    "testset_np = testset.to_numpy()\n",
    "truth_label = trainset[\"duration_label\"].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the accuracy\n",
    "def calc_accuracy(truth, predicts):\n",
    "    truth_count = 0\n",
    "    for i in range(len(predicts)):\n",
    "        if truth[i] == predicts[i]:\n",
    "            truth_count += 1\n",
    "            \n",
    "    return truth_count/len(truth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce Kaggle Submission\n",
    "def submit_file(predicts, filename):\n",
    "    id = list(range(1,10001))\n",
    "    result = pd.DataFrame({'id': id, 'duration_label': predicts})\n",
    "    result.to_csv(filename, index = False)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I use Zero-R to provide the baseline of my models' evaluation. \n",
    "Note that Zero-R is not one of the 4 models I adopted to make prediction on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero-R Baseline\n",
    "labels = trainset['duration_label'].to_numpy()\n",
    "\n",
    "labels_int = labels.astype(int)\n",
    "\n",
    "# get the most freqeunt label\n",
    "dist = np.bincount(labels_int)\n",
    "perc_dist = dist/trainset.shape[0]\n",
    "freq_int = dist.argmax()\n",
    "freq = freq_int.astype(float)\n",
    "\n",
    "# generate prediction for train\n",
    "predict_train = [freq] * trainset.shape[0]\n",
    "\n",
    "acc_train = calc_accuracy(labels, predict_train)\n",
    "\n",
    "print(\"The accuracy of 0R baseline for trainset is\", acc_train)\n",
    "\n",
    "\n",
    "# generate prediction for test\n",
    "predict_test = [freq] * testset.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL 1 MNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing name/step/ingr into tfidfvectorizer \n",
      "selecting best 20% features by chi2 \n"
     ]
    }
   ],
   "source": [
    "# tfidf preprocessing and 20% best chi2 selection for train set\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# concatenate three text\n",
    "text = (trainset['name'] + trainset['steps'] + trainset['ingredients']).to_numpy()\n",
    "for i in range(text.shape[0]):\n",
    "    text[i] = text[i].replace(\"[\", \" \")\n",
    "    text[i] = text[i].replace(\"]\", \" \")\n",
    "    text[i] = text[i].replace(\"'\", \" \")\n",
    "\n",
    "    \n",
    "print('preprocessing name/step/ingr into tfidfvectorizer ')\n",
    "X_train_txt, X_test_txt, y_train, y_test = train_test_split(text, truth_label, train_size=0.8, test_size=0.2, random_state=666)\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words = 'english')\n",
    "vectorizer = vectorizer.fit(text)\n",
    "X_train = vectorizer.transform(X_train_txt)\n",
    "X_test = vectorizer.transform(X_test_txt)\n",
    "\n",
    "\n",
    "print('selecting best 20% features by chi2 ')\n",
    "\n",
    "\n",
    "from sklearn.feature_selection import SelectPercentile, chi2\n",
    "\n",
    "select = SelectPercentile(chi2, percentile=20)\n",
    "X_train_s = select.fit_transform(X_train, y_train)\n",
    "X_test_s = select.transform(X_test)\n",
    "\n",
    "# print(X_train_s.shape)\n",
    "# print(X_test_s.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNB score\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.704375"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model 1 MultinomialNB()\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train_s, y_train)\n",
    "print(\"MNB score\")\n",
    "mnb.score(X_test_s, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing name/step/ingr into tfidfvectorizer \n",
      "selecting best 20% features by chi2 \n"
     ]
    }
   ],
   "source": [
    "# tfidf preprocessing and 20% best chi2 selection for test set\n",
    "\n",
    "text = (trainset['name'] + trainset['steps'] + trainset['ingredients']).to_numpy()\n",
    "for i in range(trainset.shape[0]):\n",
    "    text[i] = text[i].replace(\"[\", \" \")\n",
    "    text[i] = text[i].replace(\"]\", \" \")\n",
    "    text[i] = text[i].replace(\"'\", \" \")\n",
    "\n",
    "text_test = (testset['name'] + testset['steps'] + testset['ingredients']).to_numpy()\n",
    "for i in range(text_test.shape[0]):\n",
    "    text_test[i] = text_test[i].replace(\"[\", \" \")\n",
    "    text_test[i] = text_test[i].replace(\"]\", \" \")\n",
    "    text_test[i] = text_test[i].replace(\"'\", \" \")\n",
    "\n",
    "all_records = np.concatenate((text, text_test), axis = 0)\n",
    "\n",
    "print('preprocessing name/step/ingr into tfidfvectorizer ')\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words = 'english')\n",
    "vectorizer = vectorizer.fit(all_records)\n",
    "all_records = vectorizer.transform(all_records)\n",
    "X_train = vectorizer.transform(text)\n",
    "X_test = vectorizer.transform(text_test)\n",
    "\n",
    "\n",
    "# print(X_train.shape)\n",
    "# print(X_test.shape)\n",
    "\n",
    "print('selecting best 20% features by chi2 ')\n",
    "select = SelectPercentile(chi2, percentile=20)\n",
    "select = select.fit(X_train, truth_label)\n",
    "X_train_s = select.transform(X_train)\n",
    "X_test_s = select.transform(X_test)\n",
    "\n",
    "# print(X_train_s.shape)\n",
    "# print(X_test_s.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict test with MNB\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train_s, truth_label)\n",
    "out = mnb.predict(X_test_s)\n",
    "submit_file(out, 'tfidf_mnb.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL 2 SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing name/step/ingr into tfidfvectorizer \n",
      "selecting best 20% features by chi2 \n"
     ]
    }
   ],
   "source": [
    "# tfidf preprocessing and 20% best chi2 selection for train set\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# concatenate three text\n",
    "text = (trainset['name'] + trainset['steps'] + trainset['ingredients']).to_numpy()\n",
    "for i in range(text.shape[0]):\n",
    "    text[i] = text[i].replace(\"[\", \" \")\n",
    "    text[i] = text[i].replace(\"]\", \" \")\n",
    "    text[i] = text[i].replace(\"'\", \" \")\n",
    "\n",
    "    \n",
    "print('preprocessing name/step/ingr into tfidfvectorizer ')\n",
    "X_train_txt, X_test_txt, y_train, y_test = train_test_split(text, truth_label, train_size=0.8, test_size=0.2, random_state=666)\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words = 'english')\n",
    "vectorizer = vectorizer.fit(text)\n",
    "X_train = vectorizer.transform(X_train_txt)\n",
    "X_test = vectorizer.transform(X_test_txt)\n",
    "\n",
    "# vectorizer = TfidfVectorizer(stop_words = 'english')\n",
    "# X_train = vectorizer.fit_transform(X_train_txt)\n",
    "# X_test = vectorizer.transform(X_test_txt)\n",
    "\n",
    "print('selecting best 20% features by chi2 ')\n",
    "\n",
    "\n",
    "from sklearn.feature_selection import SelectPercentile, chi2\n",
    "\n",
    "select = SelectPercentile(chi2, percentile=20)\n",
    "X_train_s = select.fit_transform(X_train, y_train)\n",
    "X_test_s = select.transform(X_test)\n",
    "\n",
    "# print(X_train_s.shape)\n",
    "# print(X_test_s.shape)\n",
    "# print(y_train.shape)\n",
    "# print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc with C =  1 is 0.78425\n",
      "acc with C =  2 is 0.78225\n",
      "acc with C =  3 is 0.78125\n",
      "acc with C =  4 is 0.77975\n",
      "acc with C =  5 is 0.778375\n",
      "acc with C =  6 is 0.77725\n",
      "acc with C =  7 is 0.776375\n",
      "acc with C =  8 is 0.775875\n",
      "acc with C =  9 is 0.775\n",
      "acc with C =  10 is 0.77425\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "# find optimal hyperparameter\n",
    "# find C\n",
    "# explain multi_class\n",
    "for i in range(1,11):\n",
    "    linear_svm = svm.LinearSVC(multi_class = 'ovr', C=i, max_iter=1000);\n",
    "    linear_svm.fit(X_train_s, y_train)\n",
    "    acc = linear_svm.score(X_test_s, y_test)\n",
    "    print(\"acc with C = \",i, \"is\", acc)\n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc with max_iter =  100 is 0.78425\n",
      "acc with max_iter =  600 is 0.78425\n",
      "acc with max_iter =  1100 is 0.78425\n",
      "acc with max_iter =  1600 is 0.78425\n",
      "acc with max_iter =  2100 is 0.78425\n",
      "acc with max_iter =  2600 is 0.78425\n",
      "acc with max_iter =  3100 is 0.78425\n",
      "acc with max_iter =  3600 is 0.78425\n",
      "acc with max_iter =  4100 is 0.78425\n",
      "acc with max_iter =  4600 is 0.78425\n",
      "fail to converge when i = 10\n",
      "acc with max_iter =  20 is 0.784625\n",
      "acc with max_iter =  30 is 0.78425\n",
      "acc with max_iter =  40 is 0.784375\n",
      "acc with max_iter =  50 is 0.78425\n",
      "acc with max_iter =  60 is 0.78425\n",
      "acc with max_iter =  70 is 0.78425\n",
      "acc with max_iter =  80 is 0.78425\n",
      "acc with max_iter =  90 is 0.78425\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "for i in range(100,5000,500):\n",
    "    linear_svm = svm.LinearSVC(multi_class = 'ovr', C=1, max_iter=i);\n",
    "    linear_svm.fit(X_train_s, y_train)\n",
    "    acc = linear_svm.score(X_test_s, y_test)\n",
    "    print(\"acc with max_iter = \",i, \"is\", acc)\n",
    "\n",
    "for i in range(10,100,10):\n",
    "    if (i != 10):\n",
    "        linear_svm = svm.LinearSVC(multi_class = 'ovr', C=1, max_iter=i);\n",
    "        linear_svm.fit(X_train_s, y_train)\n",
    "        acc = linear_svm.score(X_test_s, y_test)\n",
    "        print(\"acc with max_iter = \",i, \"is\", acc)\n",
    "    else:\n",
    "        print(\"fail to converge when i = 10\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [40000, 32000]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-4b3e85770f1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# best hyperparameter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlinear_svm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinearSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmulti_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ovr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mlinear_svm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_svm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/svm/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    225\u001b[0m                              % self.C)\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m         X, y = self._validate_data(X, y, accept_sparse='csr',\n\u001b[0m\u001b[1;32m    228\u001b[0m                                    \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"C\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                                    accept_large_sparse=False)\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    430\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                           FutureWarning)\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    811\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 813\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0m\u001b[1;32m    257\u001b[0m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [40000, 32000]"
     ]
    }
   ],
   "source": [
    "# Model 2 SVM model\n",
    "from sklearn import svm\n",
    "# best hyperparameter\n",
    "linear_svm = svm.LinearSVC(multi_class = 'ovr', C=1, max_iter=1000);\n",
    "linear_svm.fit(X_train_s, y_train)\n",
    "acc = linear_svm.score(X_test_s, y_test)\n",
    "\n",
    "print(\"Linear_svm\", acc)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not run them!!! Computational cost too high\n",
    "# svmrbf = svm.SVC(kernel='rbf', gamma=0.05, C=1)\n",
    "# svmrbf.fit(X_train_s, y_train)\n",
    "# acc = svmrbf.score(X_test_s, y_test)\n",
    "# print(\"svm with rbf kernel\", acc)\n",
    "\n",
    "# svmp2 = svm.SVC(kernel='poly', degree=2, gamma='auto', C=1)\n",
    "# svmp2.fit(X_train_s, y_train)\n",
    "# acc = svmp2.score(X_test_s, y_test)\n",
    "# print(\"svm with rbf kernel\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing name/step/ingr into tfidfvectorizer \n",
      "selecting best 20% features by chi2 \n"
     ]
    }
   ],
   "source": [
    "# tfidf preprocessing and 20% best chi2 selection for test set\n",
    "\n",
    "text = (trainset['name'] + trainset['steps'] + trainset['ingredients']).to_numpy()\n",
    "for i in range(trainset.shape[0]):\n",
    "    text[i] = text[i].replace(\"[\", \" \")\n",
    "    text[i] = text[i].replace(\"]\", \" \")\n",
    "    text[i] = text[i].replace(\"'\", \" \")\n",
    "\n",
    "text_test = (testset['name'] + testset['steps'] + testset['ingredients']).to_numpy()\n",
    "for i in range(text_test.shape[0]):\n",
    "    text_test[i] = text_test[i].replace(\"[\", \" \")\n",
    "    text_test[i] = text_test[i].replace(\"]\", \" \")\n",
    "    text_test[i] = text_test[i].replace(\"'\", \" \")\n",
    "\n",
    "all_records = np.concatenate((text, text_test), axis = 0)\n",
    "\n",
    "print('preprocessing name/step/ingr into tfidfvectorizer ')\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words = 'english')\n",
    "vectorizer = vectorizer.fit(all_records)\n",
    "all_records = vectorizer.transform(all_records)\n",
    "X_train = vectorizer.transform(text)\n",
    "X_test = vectorizer.transform(text_test)\n",
    "\n",
    "\n",
    "# print(X_train.shape)\n",
    "# print(X_test.shape)\n",
    "\n",
    "print('selecting best 20% features by chi2 ')\n",
    "select = SelectPercentile(chi2, percentile=20)\n",
    "select = select.fit(X_train, truth_label)\n",
    "X_train_s = select.transform(X_train)\n",
    "X_test_s = select.transform(X_test)\n",
    "\n",
    "# print(X_train_s.shape)\n",
    "# print(X_test_s.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "linear_svm = svm.LinearSVC(multi_class = 'ovr', C=1, max_iter=10000)\n",
    "linear_svm.fit(X_train_s, truth_label)\n",
    "out = linear_svm.predict(X_test_s)\n",
    "submit_file(out, 'tfidf_linear_svc.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL 3 Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing name/step/ingr into tfidfvectorizer \n",
      "selecting best 20% features by chi2 \n",
      "(32000, 4809)\n",
      "(8000, 4809)\n",
      "(32000,)\n",
      "(8000,)\n"
     ]
    }
   ],
   "source": [
    "# tfidf preprocessing and 20% best chi2 selection for train set\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# concatenate three text\n",
    "text = (trainset['name'] + trainset['steps'] + trainset['ingredients']).to_numpy()\n",
    "for i in range(text.shape[0]):\n",
    "    text[i] = text[i].replace(\"[\", \" \")\n",
    "    text[i] = text[i].replace(\"]\", \" \")\n",
    "    text[i] = text[i].replace(\"'\", \" \")\n",
    "\n",
    "    \n",
    "print('preprocessing name/step/ingr into tfidfvectorizer ')\n",
    "X_train_txt, X_test_txt, y_train, y_test = train_test_split(text, truth_label, train_size=0.8, test_size=0.2, random_state=666)\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words = 'english')\n",
    "vectorizer = vectorizer.fit(text)\n",
    "X_train = vectorizer.transform(X_train_txt)\n",
    "X_test = vectorizer.transform(X_test_txt)\n",
    "\n",
    "# vectorizer = TfidfVectorizer(stop_words = 'english')\n",
    "# X_train = vectorizer.fit_transform(X_train_txt)\n",
    "# X_test = vectorizer.transform(X_test_txt)\n",
    "\n",
    "print('selecting best 20% features by chi2 ')\n",
    "\n",
    "\n",
    "from sklearn.feature_selection import SelectPercentile, chi2\n",
    "\n",
    "select = SelectPercentile(chi2, percentile=20)\n",
    "X_train_s = select.fit_transform(X_train, y_train)\n",
    "X_test_s = select.transform(X_test)\n",
    "\n",
    "print(X_train_s.shape)\n",
    "print(X_test_s.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fail to converge when i = 250\n",
      "When max_iter =  250.0 Accuracy: 0.787625\n",
      "When max_iter =  625.0 Accuracy: 0.787625\n",
      "When max_iter =  1562.5 Accuracy: 0.787625\n",
      "When max_iter =  3906.25 Accuracy: 0.787625\n",
      "When max_iter =  9765.625 Accuracy: 0.787625\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# Logistic model\n",
    "i = 100\n",
    "while i < 10000:\n",
    "    if (i != 100):\n",
    "        lgr = LogisticRegression(max_iter = i)\n",
    "        lgr.fit(X_train_s, y_train)   \n",
    "        print(\"When max_iter = \", i, \"Accuracy:\",lgr.score(X_test_s,y_test))\n",
    "        \n",
    "    else:\n",
    "        print(\"fail to converge when i = 250\")\n",
    "        \n",
    "    i *= 2.5\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When C =  1 Accuracy: 0.787625\n",
      "When C =  2 Accuracy: 0.791\n",
      "When C =  3 Accuracy: 0.788625\n",
      "When C =  4 Accuracy: 0.78775\n",
      "When C =  5 Accuracy: 0.785875\n",
      "When C =  6 Accuracy: 0.78475\n",
      "When C =  7 Accuracy: 0.784625\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,8):\n",
    "    lgr = LogisticRegression(C=i, max_iter = 10000)\n",
    "    lgr.fit(X_train_s, y_train)\n",
    "    print(\"When C = \", i,\"Accuracy:\",lgr.score(X_test_s,y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.791\n"
     ]
    }
   ],
   "source": [
    "lgr = LogisticRegression(C=2, max_iter = 10000)\n",
    "lgr.fit(X_train_s, y_train)\n",
    "print(\"Accuracy:\",lgr.score(X_test_s,y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing name/step/ingr into tfidfvectorizer \n",
      "selecting best 20% features by chi2 \n"
     ]
    }
   ],
   "source": [
    "# tfidf preprocessing and 20% best chi2 selection for test set\n",
    "\n",
    "text = (trainset['name'] + trainset['steps'] + trainset['ingredients']).to_numpy()\n",
    "for i in range(trainset.shape[0]):\n",
    "    text[i] = text[i].replace(\"[\", \" \")\n",
    "    text[i] = text[i].replace(\"]\", \" \")\n",
    "    text[i] = text[i].replace(\"'\", \" \")\n",
    "\n",
    "text_test = (testset['name'] + testset['steps'] + testset['ingredients']).to_numpy()\n",
    "for i in range(text_test.shape[0]):\n",
    "    text_test[i] = text_test[i].replace(\"[\", \" \")\n",
    "    text_test[i] = text_test[i].replace(\"]\", \" \")\n",
    "    text_test[i] = text_test[i].replace(\"'\", \" \")\n",
    "\n",
    "all_records = np.concatenate((text, text_test), axis = 0)\n",
    "\n",
    "print('preprocessing name/step/ingr into tfidfvectorizer ')\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words = 'english')\n",
    "vectorizer = vectorizer.fit(all_records)\n",
    "all_records = vectorizer.transform(all_records)\n",
    "X_train = vectorizer.transform(text)\n",
    "X_test = vectorizer.transform(text_test)\n",
    "\n",
    "\n",
    "# print(X_train.shape)\n",
    "# print(X_test.shape)\n",
    "\n",
    "print('selecting best 20% features by chi2 ')\n",
    "select = SelectPercentile(chi2, percentile=20)\n",
    "select = select.fit(X_train, truth_label)\n",
    "X_train_s = select.transform(X_train)\n",
    "X_test_s = select.transform(X_test)\n",
    "\n",
    "# print(X_train_s.shape)\n",
    "# print(X_test_s.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for kaggle, C=1 is optimal\n",
    "lgr = LogisticRegression(C=1, max_iter = 10000);\n",
    "lgr.fit(X_train_s, truth_label)\n",
    "out = lgr.predict(X_test_s)\n",
    "submit_file(out, 'tfidf_log_nsi.csv')\n",
    "                           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing name/step/ingr into tfidfvectorizer \n",
      "selecting best 20% features by chi2 \n",
      "(32000, 4766)\n",
      "(8000, 4766)\n",
      "(32000,)\n",
      "(8000,)\n"
     ]
    }
   ],
   "source": [
    "# try name and step only\n",
    "# tfidf preprocessing and 20% best chi2 selection for train set\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# concatenate three text\n",
    "text = (trainset['name'] + trainset['steps']).to_numpy()\n",
    "for i in range(text.shape[0]):\n",
    "    text[i] = text[i].replace(\"[\", \" \")\n",
    "    text[i] = text[i].replace(\"]\", \" \")\n",
    "    text[i] = text[i].replace(\"'\", \" \")\n",
    "\n",
    "    \n",
    "print('preprocessing name/step/ingr into tfidfvectorizer ')\n",
    "X_train_txt, X_test_txt, y_train, y_test = train_test_split(text, truth_label, train_size=0.8, test_size=0.2, random_state=666)\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words = 'english')\n",
    "vectorizer = vectorizer.fit(text)\n",
    "X_train = vectorizer.transform(X_train_txt)\n",
    "X_test = vectorizer.transform(X_test_txt)\n",
    "\n",
    "# vectorizer = TfidfVectorizer(stop_words = 'english')\n",
    "# X_train = vectorizer.fit_transform(X_train_txt)\n",
    "# X_test = vectorizer.transform(X_test_txt)\n",
    "\n",
    "print('selecting best 20% features by chi2 ')\n",
    "\n",
    "\n",
    "from sklearn.feature_selection import SelectPercentile, chi2\n",
    "\n",
    "select = SelectPercentile(chi2, percentile=20)\n",
    "X_train_s = select.fit_transform(X_train, y_train)\n",
    "X_test_s = select.transform(X_test)\n",
    "\n",
    "print(X_train_s.shape)\n",
    "print(X_test_s.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When C =  1 Accuracy: 0.788625\n",
      "When C =  2 Accuracy: 0.787875\n",
      "When C =  3 Accuracy: 0.787375\n",
      "When C =  4 Accuracy: 0.785375\n",
      "When C =  5 Accuracy: 0.78325\n",
      "When C =  6 Accuracy: 0.781625\n",
      "When C =  7 Accuracy: 0.78275\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,8):\n",
    "    lgr = LogisticRegression(C=i, max_iter = 10000)\n",
    "    lgr.fit(X_train_s, y_train)\n",
    "    print(\"When C = \", i,\"Accuracy:\",lgr.score(X_test_s,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing name/step/ingr into tfidfvectorizer \n",
      "selecting best 20% features by chi2 \n"
     ]
    }
   ],
   "source": [
    "# tfidf preprocessing and 20% best chi2 selection for test set\n",
    "\n",
    "text = (trainset['name'] + trainset['steps']).to_numpy()\n",
    "for i in range(trainset.shape[0]):\n",
    "    text[i] = text[i].replace(\"[\", \" \")\n",
    "    text[i] = text[i].replace(\"]\", \" \")\n",
    "    text[i] = text[i].replace(\"'\", \" \")\n",
    "\n",
    "text_test = (testset['name'] + testset['steps']).to_numpy()\n",
    "for i in range(text_test.shape[0]):\n",
    "    text_test[i] = text_test[i].replace(\"[\", \" \")\n",
    "    text_test[i] = text_test[i].replace(\"]\", \" \")\n",
    "    text_test[i] = text_test[i].replace(\"'\", \" \")\n",
    "\n",
    "all_records = np.concatenate((text, text_test), axis = 0)\n",
    "\n",
    "print('preprocessing name/step/ingr into tfidfvectorizer ')\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words = 'english')\n",
    "vectorizer = vectorizer.fit(all_records)\n",
    "all_records = vectorizer.transform(all_records)\n",
    "X_train = vectorizer.transform(text)\n",
    "X_test = vectorizer.transform(text_test)\n",
    "\n",
    "\n",
    "# print(X_train.shape)\n",
    "# print(X_test.shape)\n",
    "\n",
    "print('selecting best 20% features by chi2 ')\n",
    "select = SelectPercentile(chi2, percentile=20)\n",
    "select = select.fit(X_train, truth_label)\n",
    "X_train_s = select.transform(X_train)\n",
    "X_test_s = select.transform(X_test)\n",
    "\n",
    "# print(X_train_s.shape)\n",
    "# print(X_test_s.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lgr = LogisticRegression(C=1, max_iter = 10000);\n",
    "lgr.fit(X_train_s, truth_label)\n",
    "out = lgr.predict(X_test_s)\n",
    "submit_file(out, 'tfidf_log_ns.csv')\n",
    "                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
